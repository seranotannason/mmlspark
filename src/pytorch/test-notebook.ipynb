{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PyTorchEstimator import PyTorchEstimator\n",
    "from azureml.core.workspace import Workspace\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the CIFAR10 dataset as Python dictionary\n",
    "import os, tarfile, pickle\n",
    "import urllib.request\n",
    "cdnURL = \"https://amldockerdatasets.azureedge.net\"\n",
    "# Please note that this is a copy of the CIFAR10 dataset originally found here:\n",
    "# http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "dataFile = \"cifar-10-python.tar.gz\"\n",
    "dataURL = cdnURL + \"/CIFAR10/\" + dataFile\n",
    "if not os.path.isfile(dataFile):\n",
    "    urllib.request.urlretrieve(dataURL, dataFile)\n",
    "with tarfile.open(dataFile, \"r:gz\") as f:\n",
    "    test_dict = pickle.load(f.extractfile(\"cifar-10-batches-py/test_batch\"),\n",
    "                            encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the images with labels from CIFAR dataset,\n",
    "# reformat the labels using OneHotEncoder\n",
    "import array\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def reshape_image(record):\n",
    "    image, label, filename = record\n",
    "    data = [float(x) for x in image.reshape(3,32,32).flatten()]\n",
    "    return data, label, filename\n",
    "\n",
    "convert_to_double = udf(lambda x: x, ArrayType(DoubleType()))\n",
    "\n",
    "image_rdd = zip(test_dict[\"data\"], test_dict[\"labels\"], test_dict[\"filenames\"])\n",
    "image_rdd = spark.sparkContext.parallelize(image_rdd).map(reshape_image)\n",
    "\n",
    "imagesWithLabels = image_rdd.toDF([\"images\", \"labels\", \"filename\"])\n",
    "\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "imagesWithLabels = imagesWithLabels.withColumn(\n",
    "                       \"images\",\n",
    "                       list_to_vector_udf(convert_to_double(col(\"images\")))) \\\n",
    "                       .select(\"images\", \"labels\")\n",
    "\n",
    "ohe = OneHotEncoderEstimator() \\\n",
    "        .setInputCols([\"labels\"]).setOutputCols([\"tmplabels\"]) \\\n",
    "        .setDropLast(False)\n",
    "imagesWithLabels = ohe.fit(imagesWithLabels) \\\n",
    "                      .transform(imagesWithLabels) \\\n",
    "                      .select(\"images\", \"tmplabels\") \\\n",
    "                      .withColumnRenamed(\"tmplabels\", \"labels\")\n",
    "\n",
    "imagesWithLabels.printSchema()\n",
    "\n",
    "imagesWithLabels.cache()\n",
    "print(imagesWithLabels.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def array_to_string(my_list):\n",
    "    return '[' + ','.join([str(elem) for elem in my_list]) + ']'\n",
    "\n",
    "array_to_string_udf = udf(array_to_string,StringType())\n",
    "\n",
    "df = imagesWithLabels.withColumn('images',array_to_string_udf(imagesWithLabels.images))\n",
    "df = df.withColumn('labels',array_to_string_udf(df.labels))\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imagesWithLabels.write.mode('overwrite').parquet('file:///tmp/data/data.parquet')\n",
    "df.write.mode('overwrite').parquet('file:///tmp/data/str_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.pytorch import DataLoader\n",
    "from petastorm import make_batch_reader, TransformSpec\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_batch_reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b20d3b3d718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:///tmp/external_dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batch_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#with DataLoader(reader) as train_loader:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0;31m#   pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#sample = next(iter(train_loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_batch_reader' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_url = 'file:///tmp/data/str_data.parquet'\n",
    "reader = make_batch_reader(dataset_url)\n",
    "#with DataLoader(reader) as train_loader:\n",
    " #   pass\n",
    "    #sample = next(iter(train_loader))\n",
    "    #print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pq_data = pd.read_parquet('data/str_data.parquet', engine='pyarrow')\n",
    "pq_data.loc[0, 'labels']\n",
    "\n",
    "df_try = spark.read.parquet('data/data.parquet')\n",
    "pd_try = df_try.toPandas()\n",
    "type(pd_try.loc[0, 'images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
