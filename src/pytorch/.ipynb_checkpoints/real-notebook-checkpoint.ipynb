{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PyTorchEstimator import PyTorchEstimator\n",
    "from azureml.core.workspace import Workspace\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the CIFAR10 dataset as Python dictionary\n",
    "import os, tarfile, pickle\n",
    "import urllib.request\n",
    "cdnURL = \"https://amldockerdatasets.azureedge.net\"\n",
    "# Please note that this is a copy of the CIFAR10 dataset originally found here:\n",
    "# http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "dataFile = \"cifar-10-python.tar.gz\"\n",
    "dataURL = cdnURL + \"/CIFAR10/\" + dataFile\n",
    "if not os.path.isfile(dataFile):\n",
    "    urllib.request.urlretrieve(dataURL, dataFile)\n",
    "with tarfile.open(dataFile, \"r:gz\") as f:\n",
    "    test_dict = pickle.load(f.extractfile(\"cifar-10-batches-py/test_batch\"),\n",
    "                            encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- images: vector (nullable = true)\n",
      " |-- labels: vector (nullable = true)\n",
      "\n",
      "10000\n",
      "+--------------------+--------------+\n",
      "|              images|        labels|\n",
      "+--------------------+--------------+\n",
      "|[158.0,159.0,165....|(10,[3],[1.0])|\n",
      "|[235.0,231.0,232....|(10,[8],[1.0])|\n",
      "|[158.0,158.0,139....|(10,[8],[1.0])|\n",
      "|[155.0,167.0,176....|(10,[0],[1.0])|\n",
      "|[65.0,70.0,48.0,3...|(10,[6],[1.0])|\n",
      "+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the images with labels from CIFAR dataset,\n",
    "# reformat the labels using OneHotEncoder\n",
    "import array\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def reshape_image(record):\n",
    "    image, label, filename = record\n",
    "    data = [float(x) for x in image.reshape(3,32,32).flatten()]\n",
    "    return data, label, filename\n",
    "\n",
    "convert_to_double = udf(lambda x: x, ArrayType(DoubleType()))\n",
    "\n",
    "image_rdd = zip(test_dict[\"data\"], test_dict[\"labels\"], test_dict[\"filenames\"])\n",
    "image_rdd = spark.sparkContext.parallelize(image_rdd).map(reshape_image)\n",
    "\n",
    "imagesWithLabels = image_rdd.toDF([\"images\", \"labels\", \"filename\"])\n",
    "\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "imagesWithLabels = imagesWithLabels.withColumn(\n",
    "                       \"images\",\n",
    "                       list_to_vector_udf(convert_to_double(col(\"images\")))) \\\n",
    "                       .select(\"images\", \"labels\")\n",
    "\n",
    "ohe = OneHotEncoderEstimator() \\\n",
    "        .setInputCols([\"labels\"]).setOutputCols([\"tmplabels\"]) \\\n",
    "        .setDropLast(False)\n",
    "imagesWithLabels = ohe.fit(imagesWithLabels) \\\n",
    "                      .transform(imagesWithLabels) \\\n",
    "                      .select(\"images\", \"tmplabels\") \\\n",
    "                      .withColumnRenamed(\"tmplabels\", \"labels\")\n",
    "\n",
    "imagesWithLabels.printSchema()\n",
    "\n",
    "imagesWithLabels.cache()\n",
    "print(imagesWithLabels.count())\n",
    "imagesWithLabels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the images with labels into a train and test data\n",
    "train, test = imagesWithLabels.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the estimator\n",
    "workspace = Workspace('e54229a3-0e6f-40b3-82a1-ae9cda6e2b81', 'mmlspark-serano', 'playground')\n",
    "clusterName = 'train-target'\n",
    "trainingScript = 'pytorch_train.py'\n",
    "nodeCount = 1\n",
    "modelPath = 'outputs/model.pt'\n",
    "experimentName = 'pytorch-cifar'\n",
    "\n",
    "estimator = PyTorchEstimator(workspace, clusterName, trainingScript, nodeCount, modelPath, experimentName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "Uploading data/data.parquet/._SUCCESS.crc\n",
      "Uploading data/data.parquet/.part-00000-bd2a5f56-d120-4ddf-a8d5-cb4447fbd213-c000.snappy.parquet.crc\n",
      "Uploading data/data.parquet/_SUCCESS\n",
      "Uploading data/data.parquet/part-00000-bd2a5f56-d120-4ddf-a8d5-cb4447fbd213-c000.snappy.parquet\n",
      "Uploading data/dataset.parquet/._SUCCESS.crc\n",
      "Uploading data/dataset.parquet/.part-00000-9a9b9f13-1741-4e94-aa28-d0cb638ca5db-c000.snappy.parquet.crc\n",
      "Uploading data/dataset.parquet/_SUCCESS\n",
      "Uploading data/dataset.parquet/part-00000-9a9b9f13-1741-4e94-aa28-d0cb638ca5db-c000.snappy.parquet\n",
      "Uploading data/str_data.csv/._SUCCESS.crc\n",
      "Uploading data/str_data.csv/.part-00000-70cd38bb-d800-40f6-9917-0d9b56003be6-c000.csv.crc\n",
      "Uploading data/str_data.csv/_SUCCESS\n",
      "Uploading data/str_data.csv/part-00000-70cd38bb-d800-40f6-9917-0d9b56003be6-c000.csv\n",
      "Uploading data/str_data.parquet/._SUCCESS.crc\n",
      "Uploading data/str_data.parquet/.part-00000-6538cf7b-02f4-409c-bcf5-76f44e3ea537-c000.snappy.parquet.crc\n",
      "Uploading data/str_data.parquet/_SUCCESS\n",
      "Uploading data/str_data.parquet/part-00000-6538cf7b-02f4-409c-bcf5-76f44e3ea537-c000.snappy.parquet\n",
      "Uploaded data/str_data.parquet/_SUCCESS, 1 files out of an estimated total of 16\n",
      "Uploaded data/str_data.csv/_SUCCESS, 2 files out of an estimated total of 16\n",
      "Uploaded data/dataset.parquet/_SUCCESS, 3 files out of an estimated total of 16\n",
      "Uploaded data/data.parquet/.part-00000-bd2a5f56-d120-4ddf-a8d5-cb4447fbd213-c000.snappy.parquet.crc, 4 files out of an estimated total of 16\n",
      "Uploaded data/dataset.parquet/.part-00000-9a9b9f13-1741-4e94-aa28-d0cb638ca5db-c000.snappy.parquet.crc, 5 files out of an estimated total of 16\n",
      "Uploaded data/str_data.parquet/._SUCCESS.crc, 6 files out of an estimated total of 16\n",
      "Uploaded data/dataset.parquet/._SUCCESS.crc, 7 files out of an estimated total of 16\n",
      "Uploaded data/data.parquet/_SUCCESS, 8 files out of an estimated total of 16\n",
      "Uploaded data/data.parquet/._SUCCESS.crc, 9 files out of an estimated total of 16\n",
      "Uploaded data/str_data.csv/._SUCCESS.crc, 10 files out of an estimated total of 16\n",
      "Uploaded data/str_data.parquet/.part-00000-6538cf7b-02f4-409c-bcf5-76f44e3ea537-c000.snappy.parquet.crc, 11 files out of an estimated total of 16\n",
      "Uploaded data/str_data.csv/.part-00000-70cd38bb-d800-40f6-9917-0d9b56003be6-c000.csv.crc, 12 files out of an estimated total of 16\n",
      "Uploaded data/dataset.parquet/part-00000-9a9b9f13-1741-4e94-aa28-d0cb638ca5db-c000.snappy.parquet, 13 files out of an estimated total of 16\n",
      "Uploaded data/data.parquet/part-00000-bd2a5f56-d120-4ddf-a8d5-cb4447fbd213-c000.snappy.parquet, 14 files out of an estimated total of 16\n",
      "Uploaded data/str_data.parquet/part-00000-6538cf7b-02f4-409c-bcf5-76f44e3ea537-c000.snappy.parquet, 15 files out of an estimated total of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data/str_data.csv/part-00000-70cd38bb-d800-40f6-9917-0d9b56003be6-c000.csv, 16 files out of an estimated total of 16\n",
      "Job submitted!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb53e593c1964301ad0e520fcc93536a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: pytorch-cifar_1560437354_ff85bed4\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/e54229a3-0e6f-40b3-82a1-ae9cda6e2b81/resourceGroups/mmlspark-serano/providers/Microsoft.MachineLearningServices/workspaces/playground/experiments/pytorch-cifar/runs/pytorch-cifar_1560437354_ff85bed4\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "bash: /azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "bash: /azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "JAVA_HOME is not set\n",
      "\n",
      "\n",
      "The experiment failed. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.0016446113586425781 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"pytorch_train.py\", line 27, in <module>\n",
      "    sc = SparkContext('local')\n",
      "  File \"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/context.py\", line 133, in __init__\n",
      "    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n",
      "  File \"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/context.py\", line 316, in _ensure_initialized\n",
      "    SparkContext._gateway = gateway or launch_gateway(conf)\n",
      "  File \"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/java_gateway.py\", line 46, in launch_gateway\n",
      "    return _launch_gateway(conf)\n",
      "  File \"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/java_gateway.py\", line 108, in _launch_gateway\n",
      "    raise Exception(\"Java gateway process exited before sending its port number\")\n",
      "Exception: Java gateway process exited before sending its port number\n",
      "\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: pytorch-cifar_1560437354_ff85bed4\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/e54229a3-0e6f-40b3-82a1-ae9cda6e2b81/resourceGroups/mmlspark-serano/providers/Microsoft.MachineLearningServices/workspaces/playground/experiments/pytorch-cifar/runs/pytorch-cifar_1560437354_ff85bed4\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Java gateway process exited before sending its port number\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"Exception\",\n            \"message\": \"Java gateway process exited before sending its port number\",\n            \"stackTrace\": \"  File \\\"azureml-setup/context_manager_injector.py\\\", line 96, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"pytorch_train.py\\\", line 27, in <module>\\n    sc = SparkContext('local')\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/context.py\\\", line 133, in __init__\\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/context.py\\\", line 316, in _ensure_initialized\\n    SparkContext._gateway = gateway or launch_gateway(conf)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/java_gateway.py\\\", line 46, in launch_gateway\\n    return _launch_gateway(conf)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/java_gateway.py\\\", line 108, in _launch_gateway\\n    raise Exception(\\\"Java gateway process exited before sending its port number\\\")\\n\"\n        }\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-193b4b027970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/mmlspark/src/pytorch/PyTorchEstimator.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job submitted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Download PyTorch model from completed job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/conda/lib/python3.6/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mfile_handle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                     \u001b[0mwait_post_processing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_post_processing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                     raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/conda/lib/python3.6/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Java gateway process exited before sending its port number\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"Exception\",\n            \"message\": \"Java gateway process exited before sending its port number\",\n            \"stackTrace\": \"  File \\\"azureml-setup/context_manager_injector.py\\\", line 96, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"pytorch_train.py\\\", line 27, in <module>\\n    sc = SparkContext('local')\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/context.py\\\", line 133, in __init__\\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/context.py\\\", line 316, in _ensure_initialized\\n    SparkContext._gateway = gateway or launch_gateway(conf)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/java_gateway.py\\\", line 46, in launch_gateway\\n    return _launch_gateway(conf)\\n  File \\\"/azureml-envs/azureml_ff2276d1e455a6c03e7998425b41690f/lib/python3.6/site-packages/pyspark/java_gateway.py\\\", line 108, in _launch_gateway\\n    raise Exception(\\\"Java gateway process exited before sending its port number\\\")\\n\"\n        }\n    }\n}"
     ]
    }
   ],
   "source": [
    "model = estimator.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
